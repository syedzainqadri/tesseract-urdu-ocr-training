#!/usr/bin/env python3
"""
Dataset Quality Comparison Tool
==============================

This tool analyzes and compares different OCR training datasets
to help understand why some perform better than others.

Usage:
    python3 compare_datasets.py
"""

import os
import glob
from collections import Counter

def analyze_dataset(dataset_path):
    """Analyze a dataset and return quality metrics"""
    if not os.path.exists(dataset_path):
        return None
    
    # Find all ground truth files
    gt_files = glob.glob(os.path.join(dataset_path, "*.gt.txt"))
    
    if not gt_files:
        return None
    
    texts = []
    text_lengths = []
    
    # Read all ground truth texts
    for gt_file in gt_files:
        try:
            with open(gt_file, 'r', encoding='utf-8') as f:
                text = f.read().strip()
                texts.append(text)
                text_lengths.append(len(text))
        except:
            continue
    
    # Calculate metrics
    unique_texts = len(set(texts))
    total_samples = len(texts)
    text_counter = Counter(texts)
    most_common = text_counter.most_common(5)
    
    avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
    
    # Calculate diversity score (higher is better)
    diversity_score = unique_texts / total_samples if total_samples > 0 else 0
    
    return {
        'total_samples': total_samples,
        'unique_texts': unique_texts,
        'diversity_score': diversity_score,
        'avg_text_length': avg_length,
        'most_common_texts': most_common,
        'text_lengths': text_lengths
    }

def print_dataset_analysis(name, analysis):
    """Print detailed analysis of a dataset"""
    if analysis is None:
        print(f"âŒ {name}: Dataset not found or empty")
        return
    
    print(f"\nðŸ“Š {name} Analysis:")
    print(f"   Total Samples: {analysis['total_samples']:,}")
    print(f"   Unique Texts: {analysis['unique_texts']:,}")
    print(f"   Diversity Score: {analysis['diversity_score']:.3f}")
    print(f"   Avg Text Length: {analysis['avg_text_length']:.1f} chars")
    
    print(f"   Most Common Texts:")
    for text, count in analysis['most_common_texts']:
        percentage = (count / analysis['total_samples']) * 100
        print(f"     â€¢ '{text[:30]}...' - {count} times ({percentage:.1f}%)")

def compare_datasets():
    """Compare different datasets and provide recommendations"""
    print("ðŸ” OCR Dataset Quality Analysis")
    print("=" * 50)
    
    # Analyze different datasets
    datasets = {
        "Small Quality (100 pairs)": "test_dataset",
        "Large Mixed (20K)": "urdu_20k_mixed_dataset", 
        "Quality 5K (if created)": "urdu_quality_5k_dataset"
    }
    
    analyses = {}
    for name, path in datasets.items():
        analyses[name] = analyze_dataset(path)
        print_dataset_analysis(name, analyses[name])
    
    print("\n" + "=" * 50)
    print("ðŸŽ¯ QUALITY ANALYSIS RESULTS")
    print("=" * 50)
    
    # Compare and provide insights
    print("\nðŸ’¡ Key Insights:")
    
    # Check diversity scores
    valid_analyses = {k: v for k, v in analyses.items() if v is not None}
    
    if len(valid_analyses) >= 2:
        # Sort by diversity score
        sorted_by_diversity = sorted(valid_analyses.items(), 
                                   key=lambda x: x[1]['diversity_score'], 
                                   reverse=True)
        
        print(f"\nðŸ“ˆ Diversity Ranking (Higher = Better):")
        for i, (name, analysis) in enumerate(sorted_by_diversity, 1):
            score = analysis['diversity_score']
            if score > 0.8:
                quality = "ðŸŸ¢ Excellent"
            elif score > 0.5:
                quality = "ðŸŸ¡ Good"
            elif score > 0.2:
                quality = "ðŸŸ  Fair"
            else:
                quality = "ðŸ”´ Poor"
            
            print(f"   {i}. {name}: {score:.3f} {quality}")
    
    print(f"\nðŸ” Why Large Datasets Can Perform Worse:")
    print(f"   â€¢ Low diversity = Model memorizes instead of learning")
    print(f"   â€¢ Too much noise = Model learns wrong patterns")
    print(f"   â€¢ Repetitive text = Poor generalization")
    print(f"   â€¢ Quality > Quantity for OCR training")
    
    print(f"\nâœ… Recommendations:")
    print(f"   1. Use the small, high-quality dataset (100 pairs)")
    print(f"   2. Create quality dataset with: python3 create_quality_dataset.py")
    print(f"   3. Focus on diverse, clean text samples")
    print(f"   4. Minimize image distortions")
    print(f"   5. Use proper Urdu fonts")
    
    print(f"\nðŸŽ¯ Expected Performance:")
    print(f"   â€¢ Small Quality Dataset: 5-15% error rate")
    print(f"   â€¢ Large Mixed Dataset: 25-40% error rate")
    print(f"   â€¢ Quality 5K Dataset: 3-10% error rate")

if __name__ == "__main__":
    compare_datasets()
