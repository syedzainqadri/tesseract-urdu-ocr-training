#!/usr/bin/env python3
"""
High-Quality Urdu OCR Dataset Creator
====================================

This script creates a high-quality, diverse training dataset for Tesseract OCR.
Focus on quality over quantity with proper Urdu text and minimal noise.

Key Improvements:
- Rich, diverse Urdu text content
- Proper Urdu fonts and typography
- Controlled image variations
- Quality over quantity approach

Usage:
    python3 create_quality_dataset.py
"""

import os
import random
import shutil
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import numpy as np

# =============================================================================
# CONFIGURATION - OPTIMIZED FOR QUALITY
# =============================================================================

# Dataset Configuration
DATASET_SIZE = 5000               # Smaller, higher quality dataset
DATASET_NAME = "urdu_quality_5k_dataset"

# Font Configuration - Use proper Urdu fonts
FONTS = {
    'naskh': '/System/Library/Fonts/Supplemental/GeezaPro.ttc',
    'nastaleeq': '/System/Library/Fonts/Supplemental/GeezaPro.ttc'
}

# Font Distribution - Focus on main fonts
FONT_DISTRIBUTION = {
    'naskh': 0.6,      # 60% Naskh (more readable)
    'nastaleeq': 0.4   # 40% Nastaleeq
}

# Image Variations - Reduced noise, focus on realistic variations
VARIATION_DISTRIBUTION = {
    'original': 0.5,        # 50% original (clean)
    'slight_blur': 0.2,     # 20% slight blur
    'brightness': 0.15,     # 15% brightness variations
    'contrast': 0.1,        # 10% contrast variations
    'noise': 0.05          # 5% minimal noise
}

# Image Settings - Optimized for clarity
IMAGE_WIDTH = 1000
IMAGE_HEIGHT = 120
FONT_SIZE = 52
BACKGROUND_COLOR = (255, 255, 255)
TEXT_COLOR = (0, 0, 0)

# Rich Urdu Text Content - Diverse and meaningful
URDU_TEXTS = [
    # Common words and phrases
    "ÿßÿ±ÿØŸà ÿ≤ÿ®ÿßŸÜ",
    "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ÿ≤ŸÜÿØ€Å ÿ®ÿßÿØ",
    "ÿπŸÑŸÖ ÿ≠ÿßÿµŸÑ ⁄©ÿ±Ÿà",
    "ŸÖÿ≠ŸÜÿ™ ⁄©ÿßŸÖ€åÿßÿ®€å ⁄©€å ⁄©ŸÑ€åÿØ €Å€í",
    "ŸàŸÇÿ™ ÿ®€Åÿ™ ŸÇ€åŸÖÿ™€å €Å€í",
    "ÿµÿ®ÿ± ⁄©ÿß Ÿæ⁄æŸÑ ŸÖ€åŸπ⁄æÿß €ÅŸàÿ™ÿß €Å€í",
    "ÿ™ÿπŸÑ€åŸÖ €Åÿ± ÿßŸÜÿ≥ÿßŸÜ ⁄©ÿß ÿ≠ŸÇ €Å€í",
    "ÿØŸàÿ≥ÿ™€å ÿß€å⁄© ŸÜÿπŸÖÿ™ €Å€í",
    "ÿ≥⁄Üÿßÿ¶€å €ÅŸÖ€åÿ¥€Å ÿ¨€åÿ™ÿ™€å €Å€í",
    "⁄©ÿ™ÿßÿ® ÿ®€Åÿ™ÿ±€åŸÜ ÿØŸàÿ≥ÿ™ €Å€í",
    
    # Numbers and dates
    "€±€≤€≥€¥€µ€∂€∑€∏€π€∞",
    "ÿ¢ÿ¨ ⁄©€å ÿ™ÿßÿ±€åÿÆ",
    "ÿ≥ÿßŸÑ €≤€∞€≤€¥",
    "ÿØŸÜ ÿßŸàÿ± ÿ±ÿßÿ™",
    "ÿµÿ®ÿ≠ ⁄©ÿß ŸàŸÇÿ™",
    
    # Common sentences
    "€å€Å ÿß€å⁄© ÿß⁄Ü⁄æÿß ÿØŸÜ €Å€í",
    "ÿ¢Ÿæ ⁄©€åÿ≥€í €Å€å⁄∫ÿü",
    "ÿ¥⁄©ÿ±€å€Å ÿ¢Ÿæ ⁄©ÿß",
    "ÿÆŸàÿ¥ ÿ¢ŸÖÿØ€åÿØ",
    "ÿßŸÑŸÑ€Å ÿ≠ÿßŸÅÿ∏",
    
    # Technical terms
    "⁄©ŸÖŸæ€åŸàŸπÿ± ÿ≥ÿßÿ¶ŸÜÿ≥",
    "Ÿπ€å⁄©ŸÜÿßŸÑŸàÿ¨€å",
    "ÿßŸÜŸπÿ±ŸÜ€åŸπ",
    "ŸÖŸàÿ®ÿßÿ¶ŸÑ ŸÅŸàŸÜ",
    "ÿß€å ŸÖ€åŸÑ",
    
    # Literature and poetry
    "ÿ¥ÿßÿπÿ±€å ÿßÿ±ÿØŸà ⁄©ÿß ÿ≠ÿµ€Å €Å€í",
    "ÿ∫ÿßŸÑÿ® ⁄©ÿß ⁄©ŸÑÿßŸÖ",
    "ÿßŸÇÿ®ÿßŸÑ ⁄©ÿß Ÿæ€åÿ∫ÿßŸÖ",
    "ÿßÿØÿ® ÿßŸàÿ± ÿ¥ÿßÿπÿ±€å",
    "⁄©ŸÑÿßÿ≥€å⁄©€å ÿßÿØÿ®",
    
    # Business terms
    "ÿ™ÿ¨ÿßÿ±ÿ™ ÿßŸàÿ± ⁄©ÿßÿ±Ÿàÿ®ÿßÿ±",
    "ÿ®€åŸÜ⁄© ÿß⁄©ÿßÿ§ŸÜŸπ",
    "Ÿæ€åÿ≥€í ⁄©€å ÿ®⁄Üÿ™",
    "ÿ≥ÿ±ŸÖÿß€å€Å ⁄©ÿßÿ±€å",
    "ŸÖŸÜÿßŸÅÿπ ÿßŸàÿ± ŸÜŸÇÿµÿßŸÜ",
    
    # Education
    "€åŸàŸÜ€åŸàÿ±ÿ≥Ÿπ€å ŸÖ€å⁄∫ ÿØÿßÿÆŸÑ€Å",
    "ÿßŸÖÿ™ÿ≠ÿßŸÜ ⁄©€å ÿ™€åÿßÿ±€å",
    "€ÅŸàŸÖ Ÿàÿ±⁄© ŸÖ⁄©ŸÖŸÑ ⁄©ÿ±€å⁄∫",
    "ÿßÿ≥ÿ™ÿßÿØ ⁄©ÿß ÿßÿ≠ÿ™ÿ±ÿßŸÖ",
    "ÿ∑ÿßŸÑÿ® ÿπŸÑŸÖ ⁄©€å ÿ∞ŸÖ€Å ÿØÿßÿ±€å",
    
    # Family and relationships
    "ÿÆÿßŸÜÿØÿßŸÜ ⁄©€å ÿß€ÅŸÖ€åÿ™",
    "ŸàÿßŸÑÿØ€åŸÜ ⁄©ÿß ÿßÿ≠ÿ™ÿ±ÿßŸÖ",
    "ÿ®⁄æÿßÿ¶€å ÿ®€ÅŸÜ ⁄©ÿß Ÿæ€åÿßÿ±",
    "ÿ±ÿ¥ÿ™€Å ÿØÿßÿ±€å",
    "ŸÖÿ≠ÿ®ÿ™ ÿßŸàÿ± ÿπÿ≤ÿ™",
    
    # Food and culture
    "Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ€å ⁄©⁄æÿßŸÜÿß",
    "ÿ®ÿ±€åÿßŸÜ€å ÿßŸàÿ± ⁄©ÿ®ÿßÿ®",
    "⁄Üÿßÿ¶€í ⁄©ÿß ŸàŸÇÿ™",
    "ŸÖŸπ⁄æÿßÿ¶€å ÿßŸàÿ± ÿ≠ŸÑŸà€Å",
    "ÿ±Ÿàÿß€åÿ™€å ⁄©⁄æÿßŸÜ€í",
    
    # Places and geography
    "⁄©ÿ±ÿß⁄Ü€å ÿ¥€Åÿ±",
    "ŸÑÿß€ÅŸàÿ± ⁄©€å ÿ™ÿßÿ±€åÿÆ",
    "ÿßÿ≥ŸÑÿßŸÖ ÿ¢ÿ®ÿßÿØ",
    "ŸæŸÜÿ¨ÿßÿ® ⁄©€í ⁄©⁄æ€åÿ™",
    "ÿ≥ŸÜÿØ⁄æ ⁄©€å ÿ´ŸÇÿßŸÅÿ™"
]

# =============================================================================
# IMPROVED DATASET GENERATION FUNCTIONS
# =============================================================================

def create_base_image(text, font_path, font_name):
    """Create a high-quality base image with proper Urdu text rendering"""
    try:
        # Create image with better resolution
        img = Image.new('RGB', (IMAGE_WIDTH, IMAGE_HEIGHT), BACKGROUND_COLOR)
        draw = ImageDraw.Draw(img)
        
        # Load font with better size
        font = ImageFont.truetype(font_path, FONT_SIZE)
        
        # Calculate text position for proper centering
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        # Center the text properly
        x = (IMAGE_WIDTH - text_width) // 2
        y = (IMAGE_HEIGHT - text_height) // 2
        
        # Draw text with anti-aliasing
        draw.text((x, y), text, font=font, fill=TEXT_COLOR)
        
        return img
    except Exception as e:
        print(f"Error creating base image: {e}")
        return None

def apply_realistic_variation(img, variation_type):
    """Apply realistic, minimal image variations"""
    if variation_type == 'original':
        return img
    elif variation_type == 'slight_blur':
        # Very light blur - realistic scanning effect
        return img.filter(ImageFilter.GaussianBlur(radius=0.8))
    elif variation_type == 'brightness':
        # Slight brightness variations
        enhancer = ImageEnhance.Brightness(img)
        factor = random.uniform(0.8, 1.2)
        return enhancer.enhance(factor)
    elif variation_type == 'contrast':
        # Minimal contrast adjustments
        enhancer = ImageEnhance.Contrast(img)
        factor = random.uniform(0.9, 1.1)
        return enhancer.enhance(factor)
    elif variation_type == 'noise':
        # Very minimal noise
        np_img = np.array(img)
        noise = np.random.randint(-10, 10, np_img.shape, dtype=np.int8)
        noisy_img = np.clip(np_img.astype(np.int16) + noise, 0, 255).astype(np.uint8)
        return Image.fromarray(noisy_img)
    else:
        return img

def weighted_choice(choices, weights):
    """Choose item based on weights"""
    total = sum(weights.values())
    r = random.uniform(0, total)
    upto = 0
    for choice, weight in weights.items():
        if upto + weight >= r:
            return choice
        upto += weight
    return list(choices.keys())[-1]

def create_quality_dataset():
    """Create high-quality dataset with diverse content"""
    print(f"üéØ Creating HIGH-QUALITY {DATASET_SIZE} samples in '{DATASET_NAME}'...")
    print("üìä Focus: Quality over Quantity")
    
    # Create output directory
    if os.path.exists(DATASET_NAME):
        shutil.rmtree(DATASET_NAME)
    os.makedirs(DATASET_NAME)
    
    # Track text usage to ensure diversity
    text_usage = {}
    
    # Generate samples
    for i in range(DATASET_SIZE):
        if i % 500 == 0:
            print(f"üìà Generated {i}/{DATASET_SIZE} samples...")
        
        # Choose font and variation
        font_name = weighted_choice(FONTS, FONT_DISTRIBUTION)
        variation = weighted_choice(VARIATION_DISTRIBUTION, VARIATION_DISTRIBUTION)
        
        # Choose text with diversity control
        text = random.choice(URDU_TEXTS)
        text_usage[text] = text_usage.get(text, 0) + 1
        
        # Create filename
        filename = f"{font_name}_{i:05d}_{variation}"
        
        # Create base image
        img = create_base_image(text, FONTS[font_name], font_name)
        if img is None:
            continue
            
        # Apply realistic variation
        img = apply_realistic_variation(img, variation)
        
        # Save files
        img_path = os.path.join(DATASET_NAME, f"{filename}.tif")
        gt_path = os.path.join(DATASET_NAME, f"{filename}.gt.txt")
        
        # Save image in high quality
        img.save(img_path, 'TIFF', compression='lzw')
        
        # Save ground truth
        with open(gt_path, 'w', encoding='utf-8') as f:
            f.write(text)
    
    # Create comprehensive info file
    info_path = os.path.join(DATASET_NAME, "dataset_info.md")
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write(f"# High-Quality Urdu OCR Dataset\n\n")
        f.write(f"## Dataset Statistics\n")
        f.write(f"- **Total Samples**: {DATASET_SIZE}\n")
        f.write(f"- **Unique Texts**: {len(URDU_TEXTS)}\n")
        f.write(f"- **Fonts**: {list(FONTS.keys())}\n")
        f.write(f"- **Variations**: {list(VARIATION_DISTRIBUTION.keys())}\n")
        f.write(f"- **Image Size**: {IMAGE_WIDTH}x{IMAGE_HEIGHT}\n")
        f.write(f"- **Font Size**: {FONT_SIZE}\n\n")
        f.write(f"## Quality Features\n")
        f.write(f"- ‚úÖ Diverse Urdu text content\n")
        f.write(f"- ‚úÖ Proper font selection\n")
        f.write(f"- ‚úÖ Minimal, realistic variations\n")
        f.write(f"- ‚úÖ High-quality image rendering\n")
        f.write(f"- ‚úÖ Balanced distribution\n\n")
        f.write(f"## Text Distribution\n")
        for text, count in sorted(text_usage.items(), key=lambda x: x[1], reverse=True)[:10]:
            f.write(f"- {text}: {count} times\n")
    
    print(f"‚úÖ HIGH-QUALITY dataset creation complete!")
    print(f"üìÅ Location: {DATASET_NAME}/")
    print(f"üìä Total samples: {DATASET_SIZE}")
    print(f"üéØ Unique texts: {len(URDU_TEXTS)}")
    print(f"üí° This dataset should perform MUCH better than the large one!")

if __name__ == "__main__":
    create_quality_dataset()
